{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upRLxmbe57rP"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_choice = 'mnist'\n",
        "epochs = 50\n",
        "batch_size = 128\n",
        "noise_dim = 100\n",
        "learning_rate = 0.0002\n",
        "save_interval = 5\n",
        "\n",
        "print(f\"dataset_choice: {dataset_choice}\")\n",
        "print(f\"epochs: {epochs}\")\n",
        "print(f\"batch_size: {batch_size}\")\n",
        "print(f\"noise_dim: {noise_dim}\")\n",
        "print(f\"learning_rate: {learning_rate}\")\n",
        "print(f\"save_interval: {save_interval}\")"
      ],
      "metadata": {
        "id": "ajv2kcE0596h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Loading and preprocessing MNIST dataset...')\n",
        "(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Reshape to include channel dimension\n",
        "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
        "\n",
        "# Normalize the images to a range of [-1, 1]\n",
        "train_images = (train_images - 127.5) / 127.5\n",
        "\n",
        "# Create a tf.data.Dataset\n",
        "buffer_size = 60000 # MNIST dataset size\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(buffer_size).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "print('MNIST dataset loaded and preprocessed.')\n",
        "print(f\"Shape of preprocessed training images: {train_images.shape}\")\n",
        "print(f\"Batch size: {batch_size}\")\n",
        "print(f\"Number of batches: {len(train_dataset)}\")"
      ],
      "metadata": {
        "id": "T8sj2Lsp6A4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_generator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.Input(shape=(noise_dim,))) # Explicitly define input layer\n",
        "    model.add(layers.Dense(7 * 7 * 256, use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Reshape((7, 7, 256)))\n",
        "    assert model.output_shape == (None, 7, 7, 256) # None is the batch size\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 7, 7, 128)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 14, 14, 64)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', activation='tanh'))\n",
        "    assert model.output_shape == (None, 28, 28, 1)\n",
        "\n",
        "    return model\n",
        "\n",
        "generator = make_generator_model()\n",
        "print('Generator Model Summary:')\n",
        "generator.summary()"
      ],
      "metadata": {
        "id": "SFSRFvVb6DlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_discriminator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.Input(shape=(28, 28, 1))) # Explicitly define input layer\n",
        "\n",
        "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(1)) # No activation, output raw logits\n",
        "\n",
        "    return model\n",
        "\n",
        "discriminator = make_discriminator_model()\n",
        "print('Discriminator Model Summary:')\n",
        "discriminator.summary()"
      ],
      "metadata": {
        "id": "Gv3R0OAA6GZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "print('Loss functions and optimizers defined.')"
      ],
      "metadata": {
        "id": "rDeuzoBN6LJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "noise_seed = tf.random.normal([16, noise_dim]) # Seed for visualization\n",
        "\n",
        "@tf.function\n",
        "def train_step(images):\n",
        "    # 1. Generate noise for the generator input.\n",
        "    noise = tf.random.normal([batch_size, noise_dim])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        # Generate fake images using the generator.\n",
        "        generated_images = generator(noise, training=True)\n",
        "\n",
        "        # Get discriminator predictions for real and fake images.\n",
        "        real_output = discriminator(images, training=True)\n",
        "        fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "        # Calculate generator and discriminator losses.\n",
        "        gen_loss = generator_loss(fake_output)\n",
        "        disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    # Calculate gradients for the generator and apply them.\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "\n",
        "    # Calculate gradients for the discriminator and apply them.\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "\n",
        "    return gen_loss, disc_loss\n",
        "\n",
        "print('`train_step` function defined.')"
      ],
      "metadata": {
        "id": "5iJyu-RE6P97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = 'generated_samples'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "print(f\"Created directory: {output_dir}\")\n",
        "\n",
        "def generate_and_save_images(model, epoch, test_input):\n",
        "    # Notice `training` is set to False. This is so all layers run in inference mode (batchnorm).\n",
        "    predictions = model(test_input, training=False)\n",
        "\n",
        "    fig = plt.figure(figsize=(10, 10))\n",
        "\n",
        "    for i in range(predictions.shape[0]):\n",
        "        plt.subplot(5, 5, i+1)\n",
        "        # Rescale images from [-1, 1] to [0, 1]\n",
        "        plt.imshow((predictions[i, :, :, 0] * 0.5 + 0.5), cmap='gray')\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.suptitle(f'Epoch {epoch}', fontsize=16)\n",
        "    plt.savefig(os.path.join(output_dir, f'mnist_epoch_{epoch:03d}.png'))\n",
        "    plt.close(fig) # Close the figure to free up memory\n",
        "\n",
        "print('`generate_and_save_images` function defined.')"
      ],
      "metadata": {
        "id": "aX64Qo2y6Qlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Starting GAN training...')\n",
        "for epoch in range(epochs):\n",
        "    gen_losses = []\n",
        "    disc_losses = []\n",
        "\n",
        "    for batch_idx, image_batch in enumerate(train_dataset):\n",
        "        g_loss, d_loss = train_step(image_batch)\n",
        "        gen_losses.append(g_loss.numpy())\n",
        "        disc_losses.append(d_loss.numpy())\n",
        "\n",
        "    avg_gen_loss = np.mean(gen_losses)\n",
        "    avg_disc_loss = np.mean(disc_losses)\n",
        "\n",
        "    print(f'Epoch {epoch + 1:03d}, G_loss: {avg_gen_loss:.4f}, D_loss: {avg_disc_loss:.4f}')\n",
        "\n",
        "    # Generate and save images at specified intervals\n",
        "    if (epoch + 1) == 1 or (epoch + 1) == 2 or (epoch + 1) == 3 or (epoch + 1) == 4 or (epoch + 1) == 5 or (epoch + 1) % save_interval == 0:\n",
        "        generate_and_save_images(generator, epoch + 1, noise_seed)\n",
        "\n",
        "print('GAN training complete.')"
      ],
      "metadata": {
        "id": "6ApdEYLY6U67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Get a batch of real images from the training dataset\n",
        "for real_images_batch in train_dataset.take(1):\n",
        "    real_images_display = real_images_batch.numpy()\n",
        "\n",
        "# Generate a batch of fake images\n",
        "num_images_to_compare = 10 # Let's compare 10 real and 10 fake images\n",
        "noise_for_comparison = tf.random.normal([num_images_to_compare, noise_dim])\n",
        "generated_images_display = generator(noise_for_comparison, training=False).numpy()\n",
        "\n",
        "# Rescale images from [-1, 1] to [0, 1] for display\n",
        "real_images_display = (real_images_display * 0.5 + 0.5)\n",
        "generated_images_display = (generated_images_display * 0.5 + 0.5)\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "for i in range(num_images_to_compare):\n",
        "    # Display Real Images\n",
        "    plt.subplot(2, num_images_to_compare, i + 1)\n",
        "    plt.imshow(real_images_display[i, :, :, 0], cmap='gray')\n",
        "    plt.axis('off')\n",
        "    if i == 0:\n",
        "        plt.title('Real', fontsize=12)\n",
        "\n",
        "    # Display Generated Images\n",
        "    plt.subplot(2, num_images_to_compare, i + 1 + num_images_to_compare)\n",
        "    plt.imshow(generated_images_display[i, :, :, 0], cmap='gray')\n",
        "    plt.axis('off')\n",
        "    if i == 0:\n",
        "        plt.title('Generated', fontsize=12)\n",
        "\n",
        "plt.suptitle('Comparison of Real and Generated MNIST Images', fontsize=16, y=1.05)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZpwFjvvv6YHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_output_dir = 'final_generated_images'\n",
        "os.makedirs(final_output_dir, exist_ok=True)\n",
        "print(f\"Created directory: {final_output_dir}\")\n",
        "\n",
        "num_final_images = 100\n",
        "# Generate 100 random noise vectors\n",
        "final_noise = tf.random.normal([num_final_images, noise_dim])\n",
        "\n",
        "# Generate images using the trained generator\n",
        "print(f'Generating {num_final_images} final images...')\n",
        "final_generated_images = generator(final_noise, training=False)\n",
        "\n",
        "# Save each generated image individually\n",
        "for i in range(num_final_images):\n",
        "    image = final_generated_images[i, :, :, 0] # Get the single channel image\n",
        "    # Rescale images from [-1, 1] to [0, 1]\n",
        "    image = (image * 0.5 + 0.5).numpy()\n",
        "\n",
        "    plt.imshow(image, cmap='gray')\n",
        "    plt.axis('off')\n",
        "    plt.savefig(os.path.join(final_output_dir, f'final_image_{i:03d}.png'))\n",
        "    plt.close() # Close the figure to free up memory\n",
        "\n",
        "print(f'{num_final_images} final images saved to {final_output_dir}/')\n"
      ],
      "metadata": {
        "id": "q_L3ApEe6bmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Loading and preprocessing MNIST dataset for classifier...')\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Reshape images to (num_samples, 28, 28, 1) and normalize to [0, 1]\n",
        "x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
        "\n",
        "print('MNIST dataset loaded and preprocessed for classifier.')\n",
        "print(f\"x_train shape: {x_train.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"x_test shape: {x_test.shape}, y_test shape: {y_test.shape}\")"
      ],
      "metadata": {
        "id": "w1AS85AQ6eso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Building and compiling classifier model...')\n",
        "def build_classifier():\n",
        "    model = models.Sequential([\n",
        "        tf.keras.Input(shape=(28, 28, 1)), # Explicitly define input layer\n",
        "        layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dense(10, activation='softmax') # 10 output classes for MNIST\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "classifier = build_classifier()\n",
        "print('Classifier Model Summary:')\n",
        "classifier.summary()"
      ],
      "metadata": {
        "id": "BT2pWCVd6hcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Training classifier model...')\n",
        "classifier_epochs = 5\n",
        "classifier.fit(x_train, y_train, epochs=classifier_epochs, validation_data=(x_test, y_test), verbose=1)\n",
        "print(f'Classifier trained for {classifier_epochs} epochs.')"
      ],
      "metadata": {
        "id": "w6WyOoOl6kG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Predicting labels for generated images...')\n",
        "\n",
        "# Normalize generated images from [-1, 1] to [0, 1]\n",
        "normalized_generated_images = (final_generated_images * 0.5 + 0.5).numpy()\n",
        "\n",
        "# Predict labels using the trained classifier\n",
        "predictions = classifier.predict(normalized_generated_images)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Calculate the distribution of predicted labels\n",
        "label_distribution = {i: 0 for i in range(10)}\n",
        "for label in predicted_labels:\n",
        "    label_distribution[label] += 1\n",
        "\n",
        "print('\\nPredicted label distribution for generated images:')\n",
        "for label, count in label_distribution.items():\n",
        "    print(f'Digit {label}: {count} images')\n"
      ],
      "metadata": {
        "id": "jC9omGJf6mjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gqaIrb6Z6pA8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}